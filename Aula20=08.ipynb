{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuRtxZx4pmjZ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBKe3IBypvYe"
      },
      "outputs": [],
      "source": [
        "URL_BASE = \"https://pt.wikipedia.org/\"\n",
        "PAGINA_INICIAL = \"wiki/Wikip%C3%A9dia:P%C3%A1gina_principal\"\n",
        "\n",
        "resposta = requests.get(URL_BASE + PAGINA_INICIAL)\n",
        "soup = BeautifulSoup(resposta.content, \"html.parser\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkzoiYa1qiIY"
      },
      "outputs": [],
      "source": [
        "soup.title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCKmksrvqkO9",
        "outputId": "50b2e4ca-7678-4c5d-826e-9d46cca5658e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chamada 0: Obtive pagina https://pt.wikipedia.org/wiki/Wikip%C3%A9dia:P%C3%A1gina_principal\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "for i in range(1):\n",
        "  resposta = requests.get(URL_BASE + PAGINA_INICIAL)\n",
        "  print(f\"Chamada {i}: Obtive pagina {URL_BASE + PAGINA_INICIAL}\")\n",
        "  time.sleep(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dLxlVwjrPys",
        "outputId": "eb98fe3a-f4aa-4d5e-afef-ad7a67b76d43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mariah Carey\n"
          ]
        }
      ],
      "source": [
        "# Salvando página\n",
        "\n",
        "url = \"https://pt.wikipedia.org/wiki/Mariah_Carey\"\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Vitória 64; x64; rv:142.0) Gecko/20100101 Firefox/142.0\"\n",
        "}\n",
        "\n",
        "resposta = requests.get(url, headers=headers)\n",
        "soup = BeautifulSoup(resposta.content, \"html.parser\")\n",
        "\n",
        "# Obtem o titulo da pagina\n",
        "titulo = soup.find(\"span\", {\"class\": \"mw-page-title-main\"})\n",
        "titulo = titulo.text\n",
        "print(titulo)\n",
        "\n",
        "# Salva a página em um arquivo\n",
        "with open(f\"{titulo}.html\", \"w\", encoding=\"utf-8\") as arquivo:\n",
        "  conteudo = resposta.content.decode(\"utf-8\")\n",
        "  arquivo.write(conteudo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "legP0eH3rciT"
      },
      "outputs": [],
      "source": [
        "# lendo o conteúdo de uma página\n",
        "with open(f\"{titulo}.html\", \"r\", encoding=\"utf-8\") as arquivo:\n",
        "  conteudo = \"\"\n",
        "  for linha in arquivo.readlines():\n",
        "    conteudo += linha\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZQ1zhEJbA69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f6e339d-ee0e-4910-e2f6-49fb19b1b8eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acessando https://pt.wikipedia.org/wiki/Wikip%C3%A9dia:P%C3%A1gina_principal (1)\n",
            "Acessando https://pt.wikipedia.org/wiki/Wikip%C3%A9dia (2)\n",
            "Acessando https://pt.wikipedia.org/wiki/Enciclop%C3%A9dia (3)\n",
            "Acessando https://pt.wikipedia.org/wiki/Conte%C3%BAdo_livre (4)\n",
            "Acessando https://pt.wikipedia.org/wiki/L%C3%ADngua_portuguesa (5)\n",
            "Acessando https://pt.wikipedia.org/wiki/Oops!..._I_Did_It_Again (6)\n",
            "Acessando https://pt.wikipedia.org/wiki/%C3%81lbum_de_est%C3%BAdio (7)\n",
            "Acessando https://pt.wikipedia.org/wiki/Britney_Spears (8)\n",
            "Pessoa encontrada! Total: 1\n",
            "Acessando https://pt.wikipedia.org/wiki/Jive_Records (9)\n",
            "Acessando https://pt.wikipedia.org/wiki/...Baby_One_More_Time (10)\n",
            "Acessando https://pt.wikipedia.org/wiki/...Baby_One_More_Time_Tour (11)\n",
            "Acessando https://pt.wikipedia.org/wiki/Produtores_musicais (12)\n",
            "Acessando https://pt.wikipedia.org/wiki/Max_Martin (13)\n",
            "Acessando https://pt.wikipedia.org/wiki/Rami_Yacoub (14)\n",
            "Pessoa encontrada! Total: 2\n",
            "Acessando https://pt.wikipedia.org/wiki/Per_Magnusson (15)\n",
            "Pessoa encontrada! Total: 3\n",
            "Acessando https://pt.wikipedia.org/wiki/Kristian_Lundin (16)\n",
            "Acessando https://pt.wikipedia.org/wiki/Rodney_Jerkins (17)\n",
            "Acessando https://pt.wikipedia.org/wiki/Robert_John_%22Mutt%22_Lange (18)\n",
            "Acessando https://pt.wikipedia.org/wiki/All_I_Want_for_Christmas_Is_You (19)\n",
            "Acessando https://pt.wikipedia.org/wiki/M%C3%BAsica_da_Bahia (20)\n",
            "Acessando https://pt.wikipedia.org/wiki/Artur_Bernardes (21)\n",
            "Pessoa encontrada! Total: 4\n",
            "Acessando https://pt.wikipedia.org/wiki/Acidente_rodovi%C3%A1rio_em_Herat_em_2025 (22)\n",
            "Acessando https://pt.wikipedia.org/wiki/Acidente_rodovi%C3%A1rio_em_Herate_em_2025 (23)\n",
            "Acessando https://pt.wikipedia.org/wiki/Deporta%C3%A7%C3%A3o_de_afeg%C3%A3os_do_Ir%C3%A3o_em_2025 (24)\n",
            "Acessando https://pt.wikipedia.org/wiki/Herate_(prov%C3%ADncia) (25)\n",
            "Acessando https://pt.wikipedia.org/wiki/Afeganist%C3%A3o (26)\n",
            "Acessando https://pt.wikipedia.org/wiki/Enaex_Brasil (27)\n",
            "Acessando https://pt.wikipedia.org/wiki/Regi%C3%A3o_Metropolitana_de_Curitiba (28)\n",
            "Acessando https://pt.wikipedia.org/wiki/Brasil (29)\n",
            "Acessando https://pt.wikipedia.org/wiki/Miguel_Uribe_Turbay (30)\n",
            "Pessoa encontrada! Total: 5\n",
            "Acessando https://pt.wikipedia.org/wiki/Col%C3%B4mbia (31)\n",
            "Acessando https://pt.wikipedia.org/wiki/Assassinato_de_Miguel_Uribe_Turbay (32)\n",
            "Acessando https://pt.wikipedia.org/wiki/Azerbaij%C3%A3o (33)\n",
            "Acessando https://pt.wikipedia.org/wiki/Arm%C3%AAnia (34)\n",
            "Acessando https://pt.wikipedia.org/wiki/Acordo_de_paz_entre_Arm%C3%AAnia_e_Azerbaij%C3%A3o_de_2025 (35)\n",
            "Acessando https://pt.wikipedia.org/wiki/Conflito_do_Alto_Carabaque (36)\n",
            "Acessando https://pt.wikipedia.org/wiki/Guerra_de_Gaza_(2023%E2%80%93presente) (37)\n",
            "Acessando https://pt.wikipedia.org/wiki/Invas%C3%A3o_da_Ucr%C3%A2nia_pela_R%C3%BAssia_(2022%E2%80%93presente) (38)\n",
            "Acessando https://pt.wikipedia.org/wiki/Guerra_Civil_no_Sud%C3%A3o_(2023%E2%80%93presente) (39)\n",
            "Acessando https://pt.wikipedia.org/wiki/Mortes_em_2025 (40)\n",
            "Acessando https://pt.wikipedia.org/wiki/Park_Sung-soo (41)\n",
            "Acessando https://pt.wikipedia.org/wiki/Cesare_Nosiglia (42)\n",
            "Acessando https://pt.wikipedia.org/wiki/Mestre_Damasceno (43)\n",
            "Acessando https://pt.wikipedia.org/wiki/Angela_Mortimer_Barrett (44)\n",
            "Acessando https://pt.wikipedia.org/wiki/Jos%C3%A9_Lu%C3%ADs_Borges_Coelho (45)\n",
            "Acessando https://pt.wikipedia.org/wiki/Jaguar_(cartunista) (46)\n",
            "Pessoa encontrada! Total: 6\n",
            "Acessando https://pt.wikipedia.org/wiki/Lu%C3%ADs_Lucas (47)\n",
            "Acessando https://pt.wikipedia.org/wiki/Dia_da_Independ%C3%AAncia (48)\n",
            "Acessando https://pt.wikipedia.org/wiki/Mold%C3%A1via (49)\n",
            "Acessando https://pt.wikipedia.org/wiki/1939 (50)\n",
            "Acessando https://pt.wikipedia.org/wiki/Heinkel_He_178 (51)\n",
            "Acessando https://pt.wikipedia.org/wiki/Avi%C3%A3o (52)\n",
            "Acessando https://pt.wikipedia.org/wiki/Turbojato (53)\n",
            "Acessando https://pt.wikipedia.org/wiki/Primeiro_voo (54)\n",
            "Acessando https://pt.wikipedia.org/wiki/2003 (55)\n",
            "Acessando https://pt.wikipedia.org/wiki/Planeta (56)\n",
            "Acessando https://pt.wikipedia.org/wiki/Marte_(planeta) (57)\n",
            "Acessando https://pt.wikipedia.org/wiki/Terra (58)\n",
            "Acessando https://pt.wikipedia.org/wiki/Quil%C3%B4metro (59)\n",
            "Acessando https://pt.wikipedia.org/wiki/2011 (60)\n",
            "Acessando https://pt.wikipedia.org/wiki/Furac%C3%A3o_Irene (61)\n",
            "Acessando https://pt.wikipedia.org/wiki/Costa_Leste_dos_Estados_Unidos (62)\n",
            "Acessando https://pt.wikipedia.org/wiki/Georg_Wilhelm_Friedrich_Hegel (63)\n",
            "Acessando https://pt.wikipedia.org/wiki/1770 (64)\n",
            "Acessando https://pt.wikipedia.org/wiki/1831 (65)\n",
            "Acessando https://pt.wikipedia.org/wiki/Lyndon_B._Johnson (66)\n",
            "Pessoa encontrada! Total: 7\n",
            "Acessando https://pt.wikipedia.org/wiki/1908 (67)\n",
            "Acessando https://pt.wikipedia.org/wiki/1973 (68)\n",
            "Acessando https://pt.wikipedia.org/wiki/Ces%C3%A1ria_%C3%89vora (69)\n",
            "Acessando https://pt.wikipedia.org/wiki/1941 (70)\n",
            "Acessando https://pt.wikipedia.org/wiki/Mark_Webber (71)\n",
            "Acessando https://pt.wikipedia.org/wiki/1976 (72)\n",
            "Acessando https://pt.wikipedia.org/wiki/Maria_B%C3%A1rbara_de_Bragan%C3%A7a,_Rainha_de_Espanha (73)\n",
            "Pessoa encontrada! Total: 8\n",
            "Acessando https://pt.wikipedia.org/wiki/1711 (74)\n",
            "Acessando https://pt.wikipedia.org/wiki/1758 (75)\n",
            "Acessando https://pt.wikipedia.org/wiki/Le_Corbusier (76)\n",
            "Pessoa encontrada! Total: 9\n",
            "Acessando https://pt.wikipedia.org/wiki/1887 (77)\n",
            "Acessando https://pt.wikipedia.org/wiki/1965 (78)\n",
            "Acessando https://pt.wikipedia.org/wiki/Louis_Mountbatten,_1.%C2%BA_Conde_Mountbatten_da_Birm%C3%A2nia (79)\n",
            "Pessoa encontrada! Total: 10\n",
            "Acessando https://pt.wikipedia.org/wiki/1900 (80)\n",
            "Acessando https://pt.wikipedia.org/wiki/1979 (81)\n",
            "Acessando https://pt.wikipedia.org/wiki/H%C3%A9lder_C%C3%A2mara (82)\n",
            "Acessando https://pt.wikipedia.org/wiki/1909 (83)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1743339826.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mcontador_paginas\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Acessando {pagina_atual} ({contador_paginas})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import re\n",
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import deque\n",
        "\n",
        "URL_BASE = \"https://pt.wikipedia.org\"\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "MAX_PESSOAS = 1000\n",
        "\n",
        "# Fila de links a visitar (fila)\n",
        "fila_links = deque()\n",
        "visitados = set()  # Para evitar visitar a mesma página várias vezes\n",
        "link_pessoas = []\n",
        "\n",
        "# Inicializa com a página principal\n",
        "pagina_inicial = URL_BASE + \"/wiki/Wikip%C3%A9dia:P%C3%A1gina_principal\"\n",
        "fila_links.append(pagina_inicial)\n",
        "visitados.add(pagina_inicial)\n",
        "\n",
        "contador_paginas = 0\n",
        "\n",
        "while fila_links and len(link_pessoas) < MAX_PESSOAS:\n",
        "    pagina_atual = fila_links.popleft()\n",
        "    contador_paginas += 1\n",
        "\n",
        "    # Atraso para evitar sobrecarregar o servidor\n",
        "    time.sleep(0.5)\n",
        "\n",
        "    print(f\"Acessando {pagina_atual} ({contador_paginas})\")\n",
        "\n",
        "    try:\n",
        "        # Adicionei um timeout para evitar que o script trave em requisições lentas\n",
        "        response = requests.get(pagina_atual, headers=HEADERS, timeout=10)\n",
        "        # Verifica se a resposta HTTP é 200 (sucesso)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Erro de requisição para {pagina_atual}: {e}\")\n",
        "        continue\n",
        "    except Exception as e:\n",
        "        print(f\"Erro inesperado em {pagina_atual}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Extrair infobox\n",
        "    infobox = soup.find(\"table\", {\"class\": \"infobox\"})\n",
        "    if infobox:\n",
        "        # A busca por \"Nascimento\" ou \"Nascido\" é mais precisa para perfis\n",
        "        tags = [tag.get_text(strip=True) for tag in infobox.find_all(\"th\", {\"class\": \"infobox-label\"})]\n",
        "        if \"Nascimento\" in tags or \"Nascido em\" in tags:\n",
        "            # Garante que não é uma página já encontrada\n",
        "            if pagina_atual not in link_pessoas:\n",
        "                link_pessoas.append(pagina_atual)\n",
        "                print(f\"Pessoa encontrada! Total: {len(link_pessoas)}\")\n",
        "\n",
        "    # Extrair links desta página e adicionar à fila\n",
        "    # O filtro regex é crucial aqui para evitar links externos ou de metadados\n",
        "    for link in soup.find_all(\"a\", href=re.compile(\"^(/wiki/)((?!:).)*$\")):\n",
        "        href = link.get(\"href\")\n",
        "        if href:\n",
        "            url_completa = URL_BASE + href\n",
        "            if url_completa not in visitados:\n",
        "                fila_links.append(url_completa)\n",
        "                visitados.add(url_completa)\n",
        "\n",
        "print(f\"--- FIM DA EXECUÇÃO ---\")\n",
        "print(f\"Processados {contador_paginas} links. Foram encontradas {len(link_pessoas)} pessoas.\")\n",
        "print(\"Links das pessoas encontradas:\")\n",
        "for pessoa_link in link_pessoas:\n",
        "    print(pessoa_link)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}